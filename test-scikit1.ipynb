{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598247037263",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X, y = iris.data[:, :2], iris.target\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,  y,  random_state=33)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "# print(knn.predict_proba(X_test))\n",
    "# print(y_pred)\n",
    "\n",
    "# Get score\n",
    "print(knn.score(X_test, y_test))\n",
    "accuracy_score(y_pred, y_test)\n",
    "\n",
    "# Print score report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_pred=y_pred, y_true=y_test, normalize='all'))\n",
    "\n",
    "# Other metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "print(\"Mean absolute error: %s.\" % mean_absolute_error(y_test, y_pred))\n",
    "print(\"Mean squared error: %s.\" % mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 score: %s.\" % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, StandardScaler, Binarizer\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# Normalization\n",
    "normalizer = Normalizer().fit(X_train)\n",
    "X_train_normalized = normalizer.fit_transform(X_train)\n",
    "X_test_normalized = normalizer.fit_transform(X_test)\n",
    "\n",
    "# Binarization\n",
    "binarizer = Binarizer(threshold=5).fit(X_train)\n",
    "X_train_binarized = binarizer.fit_transform(X_train) \n",
    "X_test_binarized = binarizer.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# Sample X\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# Sample Y: y = 1 * x_0 + 2 * x_1 + 3\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "reg.score(X, y) # R^2\n",
    "reg.coef_ # Show coefficients: 1 and 2 in this case\n",
    "reg.intercept_ # Show y intercept\n",
    "reg.predict(np.array([[3, 5]])) # Predict y for x_0 = 3 and x_1 = 5\n",
    "reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other models\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel=\"linear\") \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "svc = SVC(C=1, kernel='linear', random_state=0)\n",
    "scores = cross_val_score(X=X, y=y, estimator=svc, cv=5)\n",
    "\n",
    "# Using other scoring metrics\n",
    "from sklearn import metrics\n",
    "scores = cross_val_score(X=X, y=y, estimator=svc, cv=10, scoring=\"f1_macro\")\n",
    "\n",
    "# Using multile scoring metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(X=X, y=y, estimator=svc, cv=10, scoring=[\"precision_macro\", \"recall_macro\"])\n",
    "print(scores['test_recall_macro'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[456, 234], [376, 732], [68, 218], [7456, 7456]])\n",
    "\n",
    "random_state = 12883823\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n",
    "for train, test in rkf.split(X):\n",
    "    print(\"%s %s\" % (train, test))\n",
    "    X[train]\n",
    "    X[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "y = y = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "for train, test in tscv.split(X):\n",
    "    print(\"%s %s\" % (train, test))\n",
    "    print(X[train])\n",
    "    print(X[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile test.txt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "classifier = make_pipeline(StandardScaler(), SVC(C=1, kernel=\"linear\"))\n",
    "cross_val_score(classifier, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import impute\n",
    "\n",
    "s = np.array([[3, 3], [4, np.nan], [5, 5], [6, 6]])\n",
    "\n",
    "imp = impute.SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n",
    "# imp.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n",
    "\n",
    "# print(imp.fit_transform([[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]))\n",
    "\n",
    "imp = impute.KNNImputer(n_neighbors=2, weights=\"uniform\", add_indicator=True)\n",
    "imp.fit_transform(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid param search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "params = {\"n_neighbors\" : np.arange(1, 3)}\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X, y = iris.data[:, :2], iris.target\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,  y,  random_state=33)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=knn, param_grid=params, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.n_neighbors) # 2 neighbors , score = 0.8027667984189722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "X, y = iris.data[:, :2], iris.target\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,  y,  random_state=33)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "params = {\"n_neighbors\": range(1,5), \"weights\": [\"uniform\", \"distance\"]}\n",
    "rsearch = RandomizedSearchCV(estimator=knn, \n",
    "                                param_distributions=params,\n",
    "                                cv=4,\n",
    "                                n_iter=8,\n",
    "                                random_state=5)\n",
    "rsearch.fit(X_train, y_train)\n",
    "print(rsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ]
}